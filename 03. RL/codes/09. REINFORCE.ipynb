{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7915b8e6",
   "metadata": {},
   "source": [
    "### REINFORCE："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7df412",
   "metadata": {},
   "source": [
    "![](assets/291.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011706a4-4eb2-466d-8640-dd0fde37493d",
   "metadata": {},
   "source": [
    "### 例子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d6cbc8-afc4-4096-b7b1-f893fb93ad13",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](assets/256.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82bf0cc-267a-407f-b59a-118ae74e68c3",
   "metadata": {},
   "source": [
    "### 上述例子代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9348c7ff-9bd7-4e24-9b5b-ca683707437d",
   "metadata": {},
   "source": [
    "例子：\n",
    "使用gym仿真库，gym官网: https://www.gymlibrary.dev/environments/toy_text/frozen_lake/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceb6df66-515e-4a09-92bd-2e4834cbd769",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: gym==0.15.4 in c:\\users\\root\\anaconda3\\lib\\site-packages (0.15.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (1.11.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (1.24.3)\n",
      "Requirement already satisfied: six in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (1.16.0)\n",
      "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (1.3.2)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (1.2.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (4.8.1.78)\n",
      "Requirement already satisfied: future in c:\\users\\root\\anaconda3\\lib\\site-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.4) (0.18.3)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: numpy in c:\\users\\root\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: torch in c:\\users\\root\\appdata\\roaming\\python\\python311\\site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\root\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\root\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\root\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\root\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\root\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\root\\anaconda3\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\root\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\root\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: tqdm in c:\\users\\root\\anaconda3\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\root\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym==0.15.4\n",
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd69e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Namedtuple for experience\n",
    "one_step_experience = namedtuple('one_step_experience', field_names=['current_observation', 'current_action', 'reward', 'next_observation'])\n",
    "\n",
    "class CustomGridWorld(gym.Env):\n",
    "    def __init__(self, grid_size=(5, 5), goal_position=(3, 2), forbidden_grids=None, action_space=5, forbidden_grids_penalty=-2, tgt_grid_reward=10):\n",
    "        super(CustomGridWorld, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.goal_position = goal_position\n",
    "        self.forbidden_grids_penalty = forbidden_grids_penalty\n",
    "        self.tgt_grid_reward = tgt_grid_reward\n",
    "        self.action_space = spaces.Discrete(action_space)\n",
    "        self.observation_space = spaces.Discrete(grid_size[0] * grid_size[1])\n",
    "        self.state = (0, 0)\n",
    "        self.done = False\n",
    "        if forbidden_grids is None:\n",
    "            forbidden_grids = [(1, 1), (1, 2), (2, 2), (3, 1), (3, 3), (4, 1)]  \n",
    "        self.forbidden_grids = set(forbidden_grids)\n",
    "\n",
    "    def _get_state(self, observation):\n",
    "        return (observation // self.grid_size[0], observation % self.grid_size[0])\n",
    "\n",
    "    def reset(self, init_observation=0):\n",
    "        self.state = self._get_state(init_observation)\n",
    "        self.done = False\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        if action == 0:  # Up\n",
    "            new_x = max(0, x - 1)\n",
    "            new_y = y\n",
    "        elif action == 1:  # Right\n",
    "            new_x = x\n",
    "            new_y = min(self.grid_size[1] - 1, y + 1)\n",
    "        elif action == 2:  # Down\n",
    "            new_x = min(self.grid_size[0] - 1, x + 1)\n",
    "            new_y = y\n",
    "        elif action == 3:  # Left\n",
    "            new_x = x\n",
    "            new_y = max(0, y - 1)\n",
    "        elif action == 4:  # Unchanged (stay in place)\n",
    "            new_x = x\n",
    "            new_y = y\n",
    "\n",
    "        if new_x < 0 or new_x >= self.grid_size[0] or new_y < 0 or new_y >= self.grid_size[1]:\n",
    "            reward = self.forbidden_grids_penalty\n",
    "            self.state = (x, y)\n",
    "        else:\n",
    "            self.state = (new_x, new_y)\n",
    "            if self.state == self.goal_position:\n",
    "                reward = self.tgt_grid_reward\n",
    "                self.done = True\n",
    "            elif self.state in self.forbidden_grids:\n",
    "                reward = self.forbidden_grids_penalty\n",
    "            else:\n",
    "                reward = 0\n",
    "\n",
    "        return self._get_observation(), reward, self.done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        grid = np.full(self.grid_size, 'F', dtype=object)\n",
    "        grid[self.goal_position] = 'G'\n",
    "        for f in self.forbidden_grids:\n",
    "            grid[f] = 'H'\n",
    "        grid[self.state] = 'A'\n",
    "        for row in grid:\n",
    "            print(' '.join(row))\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return self.state[0] * self.grid_size[1] + self.state[1]\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    def vis_policy(self, q_table):\n",
    "        self.reset()\n",
    "        self.render()\n",
    "        action_maps = {0: '↑', 1: '→', 2: '↓', 3: '←', 4: '⊙'}\n",
    "        policy = np.full(self.grid_size, '⊙', dtype=object)\n",
    "        for row in range(self.grid_size[0]):\n",
    "            for col in range(self.grid_size[1]):\n",
    "                index = row * self.grid_size[0] + col\n",
    "                action = q_table[index].argmax()\n",
    "                policy[row, col] = action_maps[action]\n",
    "        print(policy)\n",
    "\n",
    "\n",
    "class REINFORCEModel(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_layers: list[int], output_dim: int):\n",
    "        super().__init__()\n",
    "        layers = nn.ModuleList()\n",
    "        \n",
    "        in_layer = nn.Embedding(input_dim, hidden_layers[0])\n",
    "        layers.append(in_layer)\n",
    "\n",
    "        for index in range(len(hidden_layers) - 1):\n",
    "            linear = nn.Linear(hidden_layers[index], hidden_layers[index+1], bias=True)\n",
    "            activation = nn.ReLU(inplace=True)\n",
    "            layers.extend([linear, activation])\n",
    "            \n",
    "        out_layer = nn.Linear(hidden_layers[-1], output_dim)\n",
    "        softmax = nn.Softmax(dim=-1)\n",
    "        layers.extend([out_layer, softmax])\n",
    "        \n",
    "        self.reinforce = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.reinforce(x)\n",
    "\n",
    "\n",
    "class REINFORCESolver:\n",
    "    def __init__(self, grid_size: tuple, goal_position: tuple, forbidden_grids: list[tuple], action_space: int,\n",
    "                 hidden_layers: list[int], device: torch.device, lr: float=1e-3):\n",
    "        self.device = device\n",
    "        self.grid_size = grid_size\n",
    "        self.action_space = action_space\n",
    "        self._init_env(grid_size, goal_position, forbidden_grids)\n",
    "        self._init_model(hidden_layers)\n",
    "        self._init_trainer(lr)\n",
    "    \n",
    "    def _init_env(self, grid_size: tuple, goal_position: tuple, forbidden_grids: list[tuple], action_space: int=5):\n",
    "        self.env = CustomGridWorld(grid_size=grid_size, goal_position=goal_position, forbidden_grids=forbidden_grids, action_space=action_space)\n",
    "        self.n_observations = self.env.observation_space.n\n",
    "    \n",
    "    def _init_model(self, hidden_layers: list[int]):\n",
    "        self.model = REINFORCEModel(self.n_observations, hidden_layers, self.action_space).to(self.device)\n",
    "    \n",
    "    def _init_trainer(self, lr):\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr)\n",
    "        \n",
    "    def _generate_episode(self, n_steps: int, random_start: bool=False):\n",
    "        start = 0 if not random_start else random.choice(list(range(self.n_observations)))\n",
    "        current_observation = self.env.reset(start)\n",
    "        states = torch.arange(self.n_observations, dtype=torch.int32, device=self.device)\n",
    "        episode_probs = []\n",
    "        episode_rewards = []\n",
    "        for _ in range(n_steps):\n",
    "            action_probs = self.model(states)\n",
    "            current_action = np.random.choice(list(range(self.action_space)), p=action_probs.detach().cpu().numpy()[current_observation])\n",
    "            episode_probs.append(action_probs[current_observation][current_action])\n",
    "            next_observation, reward, done, _ = self.env.step(current_action)\n",
    "            episode_rewards.append(reward)\n",
    "            current_observation = next_observation\n",
    "            # if done:\n",
    "            #     break\n",
    "            \n",
    "        return episode_probs, episode_rewards\n",
    "    \n",
    "    def solve(self, n_steps: int, n_episodes: int, gamma: float, random_start: bool=True, vis_policy: bool=True):\n",
    "        self.model.train()\n",
    "        pbar = tqdm(range(n_episodes))\n",
    "        for n_episode in pbar:\n",
    "            episode_probs, episode_rewards = self._generate_episode(n_steps, random_start=random_start)\n",
    "            episode_loss = []\n",
    "            discounted_rewards = np.zeros_like(episode_rewards, dtype=np.float32)\n",
    "            running_add = 0\n",
    "            for t in reversed(range(len(episode_rewards))):\n",
    "                running_add = episode_rewards[t] + gamma * running_add\n",
    "                discounted_rewards[t] = running_add\n",
    "            \n",
    "            # Normalize the rewards\n",
    "            discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
    "\n",
    "            for t_outer in range(len(episode_rewards)):\n",
    "                episode_loss.append(-torch.log(episode_probs[t_outer]) * discounted_rewards[t_outer])\n",
    "                \n",
    "            episode_loss = torch.stack(episode_loss).mean()\n",
    "            episode_rewards_mean = torch.tensor(episode_rewards, dtype=torch.float32).mean()\n",
    "            self.optimizer.zero_grad()\n",
    "            episode_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if n_episode % log_iters == 0:\n",
    "                desc = f'Episode: {n_episode} / {n_episodes}, avg loss: {episode_loss:.3f}, avg rewards: {episode_rewards_mean:.3f}'\n",
    "                pbar.set_description(desc)\n",
    "\n",
    "        print(\"Training Done!\")\n",
    "        \n",
    "        if vis_policy:\n",
    "            print('Rendering final policy...')\n",
    "            self.vis_policy()\n",
    "        print('All done!')\n",
    "            \n",
    "    def create_fake_qtable(self):\n",
    "        self.model.eval()\n",
    "        fake_q_table = torch.zeros([self.n_observations, self.action_space], device=self.device)\n",
    "        states = torch.arange(self.n_observations, dtype=torch.int32, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(states).detach().cpu().numpy()\n",
    "            optimal_actions = q_values.argmax(1)\n",
    "            fake_q_table[range(self.n_observations), optimal_actions] = 1\n",
    "        return fake_q_table\n",
    "    \n",
    "    def vis_policy(self):\n",
    "        fake_q_table = self.create_fake_qtable()\n",
    "        self.env.vis_policy(fake_q_table.cpu().numpy())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    grid_size = (5, 5)\n",
    "    goal_position = (3, 2)\n",
    "    forbidden_grids = [(1, 1), (1, 2), (2, 2), (3, 1), (3, 3), (4, 1)]\n",
    "    action_space = 5\n",
    "    hidden_layers = [100, 100]\n",
    "    output_dim = action_space\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    lr = 1e-3\n",
    "    solver = REINFORCESolver(\n",
    "        grid_size, goal_position, forbidden_grids, \n",
    "        action_space, hidden_layers, device, lr \n",
    "    )\n",
    "    n_steps = 100000\n",
    "    n_episodes = 100\n",
    "    log_iters = 1\n",
    "    gamma = 0.98\n",
    "    random_start = True\n",
    "    vis_policy = True\n",
    "    solver.solve(n_steps, n_episodes, gamma, random_start, vis_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f842e3-eb00-4206-8aaa-877be5fed6a8",
   "metadata": {},
   "source": [
    "![](assets/result1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7510f832-072b-43aa-85ab-974cfe09ed05",
   "metadata": {},
   "source": [
    "由于运行步数过多，直接展示结果，但是这里，不知道为什么，有些位置学习的策略并不好。。。。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
