{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7915b8e6",
   "metadata": {},
   "source": [
    "### Deep Q-learning公式（off-policy版本）："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7df412",
   "metadata": {},
   "source": [
    "![](assets/253.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011706a4-4eb2-466d-8640-dd0fde37493d",
   "metadata": {},
   "source": [
    "### 例子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e1e754-c88f-4c29-87c1-6d88365d85e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](assets/255.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d6cbc8-afc4-4096-b7b1-f893fb93ad13",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](assets/256.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82bf0cc-267a-407f-b59a-118ae74e68c3",
   "metadata": {},
   "source": [
    "### 上述例子代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9348c7ff-9bd7-4e24-9b5b-ca683707437d",
   "metadata": {},
   "source": [
    "例子：\n",
    "使用gym仿真库，gym官网: https://www.gymlibrary.dev/environments/toy_text/frozen_lake/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceb6df66-515e-4a09-92bd-2e4834cbd769",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: gym==0.15.4 in c:\\users\\root\\anaconda3\\lib\\site-packages (0.15.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (1.11.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (1.24.3)\n",
      "Requirement already satisfied: six in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (1.16.0)\n",
      "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (1.3.2)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (1.2.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (4.8.1.78)\n",
      "Requirement already satisfied: future in c:\\users\\root\\anaconda3\\lib\\site-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.4) (0.18.3)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: numpy in c:\\users\\root\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: torch in c:\\users\\root\\appdata\\roaming\\python\\python311\\site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\root\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\root\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\root\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\root\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\root\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\root\\anaconda3\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\root\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\root\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: tqdm in c:\\users\\root\\anaconda3\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\root\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym==0.15.4\n",
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd69e98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A F F F F\n",
      "F H H F F\n",
      "F F H F F\n",
      "F H G H F\n",
      "F H F F F\n",
      "start generating 1 episodes with 1000 steps......\n",
      "done\n",
      "start training for 200 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 200 / 200, average loss: 0.051: 100%|████████████████████████████████████████████| 200/200 [00:14<00:00, 13.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "render final policy...\n",
      "A F F F F\n",
      "F H H F F\n",
      "F F H F F\n",
      "F H G H F\n",
      "F H F F F\n",
      "[['↓' '→' '↓' '↓' '←']\n",
      " ['↓' '↓' '↓' '↓' '←']\n",
      " ['→' '→' '↓' '↓' '←']\n",
      " ['→' '→' '⊙' '←' '←']\n",
      " ['↑' '→' '↑' '←' '←']]\n",
      "All done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.envs.registration import register\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "one_step_experience = namedtuple('one_step_experience', field_names=['current_observation', 'current_action', 'reward', 'next_observation'])\n",
    "\n",
    "class CustomGridWorld(gym.Env):\n",
    "    def __init__(self, grid_size=(5, 5), goal_position=(3, 2), forbidden_grids=None, action_space=5):\n",
    "        super(CustomGridWorld, self).__init__()\n",
    "        # Grid size (rows, columns)\n",
    "        self.grid_size = grid_size\n",
    "        self.goal_position = goal_position\n",
    "        # Define action space: up, right, down, left, unchanged (5 actions)\n",
    "        self.action_space = spaces.Discrete(action_space)\n",
    "        # Observation space: grid positions, represented as a flat space\n",
    "        self.observation_space = spaces.Discrete(grid_size[0] * grid_size[1])\n",
    "        # Initialize agent's starting position (top-left corner)\n",
    "        self.state = (0, 0)\n",
    "        self.done = False\n",
    "        # Set the forbidden grids (if not specified, use a default list)\n",
    "        if forbidden_grids is None:\n",
    "            forbidden_grids = [(1, 1), (1, 2), (2, 2), (3, 1), (3, 3), (4, 1)]  # Given forbidden grids\n",
    "        self.forbidden_grids = set(forbidden_grids)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment to the initial state\"\"\"\n",
    "        self.state = (0, 0)  # Reset the agent to the top-left corner\n",
    "        self.done = False\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Executes one step in the environment\"\"\"\n",
    "        # if self.done:\n",
    "        #     return self._get_observation(), 1, True, {}\n",
    "\n",
    "        x, y = self.state\n",
    "        # Define movement based on action\n",
    "        if action == 0:  # Up\n",
    "            new_x = max(0, x - 1)\n",
    "            new_y = y\n",
    "        elif action == 1:  # Right\n",
    "            new_x = x\n",
    "            new_y = min(self.grid_size[1] - 1, y + 1)\n",
    "        elif action == 2:  # Down\n",
    "            new_x = min(self.grid_size[0] - 1, x + 1)\n",
    "            new_y = y\n",
    "        elif action == 3:  # Left\n",
    "            new_x = x\n",
    "            new_y = max(0, y - 1)\n",
    "        elif action == 4:  # Unchanged (stay in the same position)\n",
    "            new_x = x\n",
    "            new_y = y\n",
    "\n",
    "        # Check if the new position is out of bounds\n",
    "        if new_x < 0 or new_x >= self.grid_size[0] or new_y < 0 or new_y >= self.grid_size[1]:\n",
    "            reward = -1  # Penalty for trying to go out of bounds\n",
    "            self.state = (x, y)  # Keep the agent at the same position\n",
    "        else:\n",
    "            self.state = (new_x, new_y)\n",
    "            # Check if the agent reached the goal\n",
    "            if self.state == self.goal_position:\n",
    "                reward = 1\n",
    "                self.done = True\n",
    "            # Check if the agent stepped into a forbidden grid\n",
    "            elif self.state in self.forbidden_grids:\n",
    "                reward = -1  # Penalty for entering a forbidden grid\n",
    "            else:\n",
    "                reward = 0  # No penalty for regular move\n",
    "\n",
    "        return self._get_observation(), reward, self.done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Renders the environment (prints the grid)\"\"\"\n",
    "        grid = np.full(self.grid_size, 'F', dtype=object)  # Default is frozen\n",
    "        # Set goal, forbidden grids, and agent position\n",
    "        grid[self.goal_position] = 'G'\n",
    "        for f in self.forbidden_grids:\n",
    "            grid[f] = 'H'  # H for hole (forbidden grid)\n",
    "        # Print the grid with agent position\n",
    "        grid[self.state] = 'A'\n",
    "        for row in grid:\n",
    "            print(' '.join(row))\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"Returns the current state as a flat index\"\"\"\n",
    "        return self.state[0] * self.grid_size[1] + self.state[1]\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the environment\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def vis_policy(self, q_table):\n",
    "        self.reset()\n",
    "        self.render()\n",
    "        action_maps = {0: '↑', 1: '→', 2: '↓', 3: '←', 4: '⊙'}\n",
    "        policy = np.full(self.grid_size, '⊙', dtype=object)\n",
    "        for row in range(self.grid_size[0]):\n",
    "            for col in range(self.grid_size[1]):\n",
    "                index = row * self.grid_size[0] + col\n",
    "                action = q_table[index].argmax()\n",
    "                policy[row, col] = action_maps[action]\n",
    "        print(policy)\n",
    "        \n",
    "        \n",
    "class DQNModel(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_layers: list[int], output_dim: int):\n",
    "        super().__init__()\n",
    "        layers = nn.ModuleList()\n",
    "        layer_dims = [input_dim] + hidden_layers\n",
    "\n",
    "        for index in range(len(layer_dims) - 1):\n",
    "            linear = nn.Linear(layer_dims[index], layer_dims[index+1], bias=True)\n",
    "            activation = nn.ReLU(inplace=True)\n",
    "            layers.extend([linear, activation])\n",
    "        out_linear = nn.Linear(layer_dims[-1], output_dim)\n",
    "        layers.append(out_linear)\n",
    "        \n",
    "        self.dqn = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dqn(x)\n",
    "    \n",
    "\n",
    "class DQNDataset(Dataset):\n",
    "    def __init__(self, replay_buffer, n_observations):\n",
    "        super().__init__()\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.n_observations = n_observations\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.replay_buffer)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        state = torch.tensor(self.replay_buffer[index].current_observation, dtype=torch.int64)\n",
    "        state = F.one_hot(state, self.n_observations).to(torch.float32)\n",
    "        action = torch.tensor(self.replay_buffer[index].current_action, dtype=torch.int)\n",
    "        reward = torch.tensor(self.replay_buffer[index].reward, dtype=torch.float32)[None]\n",
    "        state_prime = torch.tensor(self.replay_buffer[index].next_observation, dtype=torch.int64)\n",
    "        state_prime = F.one_hot(state_prime, self.n_observations).to(torch.float32)\n",
    "        return state, action, reward, state_prime\n",
    "    \n",
    "        \n",
    "class DQNSolver:\n",
    "    def __init__(\n",
    "        self, grid_size: tuple, goal_position: tuple, forbidden_grids: list[tuple], action_space: int,\n",
    "        hidden_layers: list[int], device: torch.device, batch_size: int, lr: float=1e-3\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.grid_size = grid_size\n",
    "        self.action_space = action_space\n",
    "        self._init_env(grid_size, goal_position, forbidden_grids)\n",
    "        self._init_model(hidden_layers)\n",
    "        self._init_trainer(lr)\n",
    "    \n",
    "    def _init_env(self, grid_size: tuple, goal_position: tuple, forbidden_grids: list[tuple], action_space: int=5):\n",
    "        self.env = CustomGridWorld(grid_size=grid_size, goal_position=goal_position, forbidden_grids=forbidden_grids, action_space=action_space)\n",
    "        self.env.render()\n",
    "        self.n_observations = self.env.observation_space.n\n",
    "    \n",
    "    def _init_model(self, hidden_layers: list[int]):\n",
    "        self.model = DQNModel(self.n_observations, hidden_layers, self.action_space).to(self.device)\n",
    "        self.model_y = deepcopy(self.model)\n",
    "        self._disable_grad(self.model_y)\n",
    "    \n",
    "    def _init_trainer(self, lr):\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr)\n",
    "    \n",
    "    def _disable_grad(self, model):\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "    def _generate_episode(self, episode_one: list, n_steps: int):\n",
    "        current_observation = self.env.reset()\n",
    "        for _ in range(n_steps):\n",
    "            current_action = self.env.action_space.sample()\n",
    "            next_observation, reward, _, _ = self.env.step(current_action)\n",
    "            ose = one_step_experience(current_observation=current_observation, current_action=current_action, reward=reward, next_observation=next_observation)\n",
    "            current_observation = next_observation\n",
    "            episode_one.append(ose)\n",
    "    \n",
    "    def _create_dataloader(self, replay_buffer):\n",
    "        dataset = DQNDataset(replay_buffer, self.n_observations)\n",
    "        return DataLoader(dataset, self.batch_size, shuffle=True, pin_memory=self.device is torch.device('cuda'), drop_last=True)\n",
    "    \n",
    "    def solve(self, n_steps: int, n_episodes: int, n_epochs: int, C_iters: int, log_iters: int, gamma: float, vis_policy: bool=True):\n",
    "        replay_buffer = []\n",
    "        print(f'start generating {n_episodes} episodes with {n_steps} steps......')\n",
    "        for _ in range(n_episodes):\n",
    "            episode_one = []\n",
    "            self._generate_episode(episode_one, n_steps)\n",
    "            replay_buffer += episode_one\n",
    "        print('done')\n",
    "        \n",
    "        dataloader = self._create_dataloader(replay_buffer)\n",
    "        \n",
    "        gamma = torch.tensor(gamma, dtype=torch.float32, device=self.device)\n",
    "        total_iters = 0\n",
    "        total_loss = 0\n",
    "        self.model.train()\n",
    "        print(f'start training for {n_epochs} epochs')\n",
    "        pbar = tqdm(range(n_epochs))\n",
    "        for epoch in pbar:\n",
    "            for states, actions, rewards, states_prime in dataloader:\n",
    "                total_iters += 1\n",
    "                states = states.to(self.device)\n",
    "                actions = actions.to(self.device)\n",
    "                rewards = rewards.to(self.device)\n",
    "                states_prime = states_prime.to(self.device)\n",
    "                # compute yT\n",
    "                y_T = rewards + gamma * torch.max(self.model_y(states_prime), dim=-1, keepdim=True)[0]\n",
    "                y_pred = self.model(states)[range(self.batch_size), actions][..., None]\n",
    "                # compute loss\n",
    "                loss = self.loss_fn(y_pred, y_T)\n",
    "                total_loss += loss.item()\n",
    "                # zero grad\n",
    "                self.optimizer.zero_grad()\n",
    "                # backward\n",
    "                loss.backward()\n",
    "                # step\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                if total_iters % C_iters:\n",
    "                    self.model_y = deepcopy(self.model)\n",
    "                    self._disable_grad(self.model_y)\n",
    "                \n",
    "                if total_iters % log_iters == 0:\n",
    "                    desc = f'Epoch: {epoch + 1} / {n_epochs}, average loss: {total_loss / total_iters:.3f}'\n",
    "                    pbar.set_description(desc)\n",
    "        print('done')\n",
    "        \n",
    "        if vis_policy:\n",
    "            print('render final policy...')\n",
    "            self.vis_policy()\n",
    "        print('All done!')\n",
    "    \n",
    "    def create_fake_qtable(self):\n",
    "        self.model.eval()\n",
    "        fake_q_table = torch.zeros([self.n_observations, self.action_space], device=self.device)\n",
    "        for state in range(self.n_observations):\n",
    "            state_input = torch.tensor(state, dtype=torch.int64, device=self.device)\n",
    "            state_input = F.one_hot(state_input, self.n_observations).to(torch.float32)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.model(state_input).squeeze(0).detach().cpu().numpy()\n",
    "                optimal_action = q_values.argmax(0)\n",
    "                fake_q_table[state, optimal_action] = 1\n",
    "        return fake_q_table\n",
    "                \n",
    "    \n",
    "    def vis_policy(self):\n",
    "        fake_q_table = self.create_fake_qtable()\n",
    "        self.env.vis_policy(fake_q_table.cpu().numpy())\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    grid_size = (5, 5)\n",
    "    goal_position = (3, 2)\n",
    "    forbidden_grids = [(1, 1), (1, 2), (2, 2), (3, 1), (3, 3), (4, 1)]\n",
    "    action_space = 5\n",
    "    hidden_layers = [100]\n",
    "    output_dim = action_space\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    batch_size = 128\n",
    "    lr = 1e-3\n",
    "    solver = DQNSolver(\n",
    "        grid_size, goal_position, forbidden_grids, action_space,\n",
    "        hidden_layers, device, batch_size, lr \n",
    "    )\n",
    "    n_steps = 1000\n",
    "    n_episodes = 1\n",
    "    n_epochs = 200\n",
    "    C_iters = 10\n",
    "    log_iters = 200\n",
    "    gamma = 0.99\n",
    "    vis_policy = True\n",
    "    solver.solve(n_steps, n_episodes, n_epochs, C_iters, log_iters, gamma, vis_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd54bc0-b092-479a-9d31-a2c8eda9052a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
